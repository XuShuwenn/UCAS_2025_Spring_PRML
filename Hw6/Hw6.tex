\documentclass{article}
\usepackage{xeCJK} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx} 
\usepackage{tikz}

\setCJKmainfont{SimSun}
\renewcommand\CJKfamilydefault{\CJKrmdefault}
\usepackage{sectsty}
\sectionfont{\normalfont}

\usetikzlibrary{shapes.geometric, arrows}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=red!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!20]
\tikzstyle{decision} = [diamond, minimum width=3.5cm, minimum height=1cm, text centered, draw=black, fill=green!20]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{document}
\title{PRML第五次作业}
\author{许书闻 2023K8009926005}
\maketitle
\section*{第1题}

\subsection*{(a) 前向概率 \(\alpha_t(s)\)}

前向概率 \(\alpha_t(s) = P(O_1, O_2, \dots, O_t, s_t = s | \lambda)=b_j(y_t)\times \sum_{i=1}^{n}\alpha_{t-1}(i)a_{ij}\).
观测序列为\(O_t=[walk,shop,clean]\).

\textbf{\( t=1 \), \( O_1 = \text{walk} \)}: 
\[
\alpha_1(s) = \pi(s) \cdot P(O_1 = \text{walk} | s)
\]
\begin{itemize}
    \item \(\alpha_1(\text{Rainy}) = 0.6 \cdot 0.1 = 0.06\)
    \item \(\alpha_1(\text{Sunny}) = 0.4 \cdot 0.6 = 0.24\)
\end{itemize}

\textbf{\( t=2 \), \( O_2 = \text{shop} \)}: 
\[
\alpha_2(s) = \left( \sum_{s'} \alpha_{t-1}(s') \cdot P(s | s') \right) \cdot P(O_t | s)
\]
\begin{itemize}
    \item \(\alpha_2(\text{Rainy}) = [0.06 \cdot 0.7 + 0.24 \cdot 0.4] \cdot 0.4 = 0.138 \cdot 0.4 = 0.0552\)
    \item \(\alpha_2(\text{Sunny}) = [0.06 \cdot 0.3 + 0.24 \cdot 0.6] \cdot 0.3 = 0.162 \cdot 0.3 = 0.0486\)
\end{itemize}

\textbf{\( t=3 \), \( O_3 = \text{clean} \)}: 
\begin{itemize}
    \item \(\alpha_3(\text{Rainy}) = [0.0552 \cdot 0.7 + 0.0486 \cdot 0.4] \cdot 0.5 = 0.05808 \cdot 0.5 = 0.02904\)
    \item \(\alpha_3(\text{Sunny}) = [0.0552 \cdot 0.3 + 0.0486 \cdot 0.6] \cdot 0.1 = 0.04572 \cdot 0.1 = 0.004572\)
\end{itemize}

\textbf{计算 \( P(O) \)}:
\[
P(O) = \sum_{s \in S} \alpha_3(s) = \alpha_3(\text{Rainy}) + \alpha_3(\text{Sunny}) = 0.02904 + 0.004572 = 0.033612
\]

\subsection*{(b) 后向概率 \(\beta_t(s)\)}

后向概率 \(\beta_t(s) = P(O_{t+1}, O_{t+2}, \dots, O_T | s_t = s, \lambda)=\sum_{s'} P(s' | s) \cdot P(O_{t+1} | s') \cdot \beta_{t+1}(s')
\):

\textbf{\( t=3 \)}: 
\[
\beta_3(s) = 1
\]
\begin{itemize}
    \item \(\beta_3(\text{Rainy}) = 1\)
    \item \(\beta_3(\text{Sunny}) = 1\)
\end{itemize}

\textbf{\( t=2 \), \( O_3 = \text{clean} \)}: 
\[
\beta_t(s) = \sum_{s'} P(s' | s) \cdot P(O_{t+1} | s') \cdot \beta_{t+1}(s')
\]
\begin{itemize}
    \item \(\beta_2(\text{Rainy}) = 0.7 \cdot 0.5 \cdot 1 + 0.3 \cdot 0.1 \cdot 1 = 0.38\)
    \item \(\beta_2(\text{Sunny}) = 0.4 \cdot 0.5 \cdot 1 + 0.6 \cdot 0.1 \cdot 1 = 0.26\)
\end{itemize}

\textbf{\( t=1 \), \( O_2 = \text{shop} \)}: 
\begin{itemize}
    \item \(\beta_1(\text{Rainy}) = 0.7 \cdot 0.4 \cdot 0.38 + 0.3 \cdot 0.3 \cdot 0.26 = 0.1298\)
    \item \(\beta_1(\text{Sunny}) = 0.4 \cdot 0.4 \cdot 0.38 + 0.6 \cdot 0.3 \cdot 0.26 = 0.1076\)
\end{itemize}

\subsection*{(c) t=2的后验概率 \(P(s_t|O)\)}

使用前向-后向算法：
\[
P(s_t | O) = \frac{\alpha_t(s) \cdot \beta_t(s)}{\sum_{s'} \alpha_t(s') \cdot \beta_t(s')}
\]

已知：
\begin{itemize}
    \item \(\alpha_2(\text{Rainy}) = 0.0552\), \(\alpha_2(\text{Sunny}) = 0.0486\)
    \item \(\beta_2(\text{Rainy}) = 0.38\), \(\beta_2(\text{Sunny}) = 0.26\)
\end{itemize}

分母：
\[
\alpha_2(\text{Rainy}) \cdot \beta_2(\text{Rainy}) + \alpha_2(\text{Sunny}) \cdot \beta_2(\text{Sunny}) = 0.0552 \cdot 0.38 + 0.0486 \cdot 0.26 = 0.020976 + 0.012636 = 0.033612
\]

状态概率：
\begin{itemize}
    \item \( P(s_2 = \text{Rainy} | O) = \frac{0.0552 \cdot 0.38}{0.033612} = \frac{0.020976}{0.033612} \approx 0.6240 \)
    \item \( P(s_2 = \text{Sunny} | O) = \frac{0.0486 \cdot 0.26}{0.033612} = \frac{0.012636}{0.033612} \approx 0.3760 \)
\end{itemize}

\section*{第2题}

\subsection*{(a)条件集}

观察图结构：
从 \( A \) 到 \( E \) 的唯一路径是 \( A \rightarrow B \rightarrow C \rightarrow D \rightarrow E \)。
路径上的节点集合为 \( \{B, C, D\} \)。

根据分离定理，若给定 \( \{B, C, D\} \)，则 \( A \) 和 \( E \) 被完全分离（因为 \( \{B, C, D\} \) 包含路径上的所有中间节点）。因此，\( A \) 和 \( E \) 在条件集 \( \{B, C, D\} \) 下是条件独立的。

\textbf{结论}：是，条件集为 \( \{B, C, D\} \)。

\subsection*{(b) 联合概率分布的因子分解形式}

马尔可夫随机场的联合概率分布可以分解为无向图中最大团（clique）的因子乘积。给定边的集合 \( \{(A,B), (B,C), (C,D), (D,E)\} \)，我们需要识别图中的最大团。

最大团是图中无法再扩展的完全子图。

分析边：\\
  - \( (A,B) \) 形成团 \( \{A, B\} \)。\\
  - \( (B,C) \) 形成团 \( \{B, C\} \)。\\
  - \( (C,D) \) 形成团 \( \{C, D\} \)。\\
  - \( (D,E) \) 形成团 \( \{D, E\} \)。

没有更大的完全子图（例如 \( \{A, B, C\} \) 不是团，因为 \( A \) 和 \( C \) 无边）。

因此，最大团为 \( \{A, B\} \)、\( \{B, C\} \)、\( \{C, D\} \)、\( \{D, E\} \)。

联合概率分布的因子分解形式为：
\[
P(A, B, C, D, E) = \frac{1}{Z} \cdot \psi_{A,B}(A, B) \cdot \psi_{B,C}(B, C) \cdot \psi_{C,D}(C, D) \cdot \psi_{D,E}(D, E)
\]

\section*{第3题}


\subsection*{线性链条件随机场的定义}

线性链条件随机场是一种用于序列标注的判别式概率模型。它定义了一个条件概率分布 \( P(Y|X) \)，其中 \( X = \{x_1, x_2, \dots, x_T\} \) 是观测序列，\( Y = \{y_1, y_2, \dots, y_T\} \) 是对应的标签序列，且 \( Y \) 具有线性链结构（即每个 \( y_t \) 仅依赖于其前一个标签 \( y_{t-1} \) 和当前观测 \( x_t \)）。

其概率分布形式为：
\[
P(Y|X) = \frac{1}{Z(X)} \exp \left(\sum_{j} \sum_{t=1}^T \lambda_k f_k(y_t, y_{t-1}, X, t) +\sum_{k}\sum_{t=1}^{T}\mu_ks_k(y_{t},X,t)\right)
\]
其中：\\
- \( f_k(y_t, y_{t-1}, X, t) \) 是定义在观测序列两个相邻标记位置上的转移特征函数，捕获相邻标签之间的关系和观测对它们的影响。\\
- \(s_k(y_{t},X,t)\)是定义在观测序列标记位置i上的状态特征函数，用于刻画观测序列对标记变量的影响。\\
- \( \lambda_k \) 是特征函数的权重。\\
- \( Z(X) \) 是归一化常数。\\

\subsection*{线性链 CRF 与隐马尔可夫模型（HMM）的主要区别}

\begin{itemize}
    \item \textbf{模型类型}：
    \begin{itemize}
        \item HMM 是生成式模型，建模联合分布 \( P(X, Y) \)，通过 \( P(X|Y)P(Y) \) 间接计算 \( P(Y|X) \)。
        \item CRF 是判别式模型，直接建模条件分布 \( P(Y|X) \)，无需假设 \( X \) 的分布。
    \end{itemize}
    \item \textbf{特征能力}：
    \begin{itemize}
        \item HMM 依赖局部独立性假设（如观测 \( x_t \) 仅依赖当前状态 \( y_t \)），特征能力有限。
        \item CRF 使用全局特征函数，可以捕获观测序列 \( X \) 的任意特征（如上下文特征），更灵活。
    \end{itemize}
\end{itemize}

\subsection*{NER 任务中特征函数的构造示例}

在命名实体识别（NER）任务中，假设观测序列 \( X \) 是词序列，标签序列 \( Y \) 是实体标签（如 \texttt{PER} 表示人物，\texttt{LOC} 表示地点，\texttt{O} 表示非实体）。特征函数可以基于以下方式构造：

\begin{itemize}
    \item \textbf{转移特征}：捕获标签之间的依赖关系。例如：
    \[
    f_1(y_t, y_{t-1}, X, t) = \begin{cases} 
    1 & \text{如果 } y_{t-1} = \texttt{O} \text{ 且 } y_t = \texttt{PER} \\
    0 & \text{否则}
    \end{cases}
    \]
    表示从非实体到人物标签的转移。

    \item \textbf{观测特征}：捕获当前词和标签的关系。例如：
    \[
    f_2(y_t, y_{t-1}, X, t) = \begin{cases} 
    1 & \text{如果 } y_t = \texttt{LOC} \text{ 且 } x_t = \text{“北京”} \\
    0 & \text{否则}
    \end{cases}
    \]
    表示词“北京”更可能被标记为地点。

    \item \textbf{上下文特征}：捕获上下文信息。例如：
    \[
    f_3(y_t, y_{t-1}, X, t) = \begin{cases} 
    1 & \text{如果 } y_t = \texttt{DATE} \text{ 且 } x_{t-1} = \text{“在”} \\
    0 & \text{否则}
    \end{cases}
    \]
    表示如果前一个词是“在”，当前词更可能被标记为日期。

    \item \textbf{词性特征}：结合词性信息。例如：
    \[
    f_4(y_t, y_{t-1}, X, t) = \begin{cases} 
    1 & \text{如果 } y_t = \texttt{PER} \text{ 且 } \text{词性}(x_t) = \text{“人名”} \\
    0 & \text{否则}
    \end{cases}
    \]
    表示如果词性为人名，则更可能被标记为人物。
\end{itemize}

这些特征函数通过训练数据学习权重 \( \lambda_k \)，从而优化序列标注的准确性。



\section*{第4题}


\textbf{因子定义}：
\begin{itemize}
    \item \( \phi_1(A) = P(A) \)
    \item \( \phi_2(B, A) = P(B|A) \)
    \item \( \phi_3(C, A) = P(C|A) \)
    \item \( \phi_4(D, B, C) = P(D|B,C) \)，
    
    由题意，限制 \( D=1 \) 后， \( \phi_4(D=1, B, C) = P(D=1|B,C) \)
\end{itemize}

\textbf{消去 \( B \)}：
\[
\phi_5(A, C) = \sum_{B} \phi_2(B, A) \cdot \phi_4(D=1, B, C) = \sum_{B} P(B|A) \cdot P(D=1|B,C)
\]
由表知，

\(P(B=1|A=1)=\frac{P(A=1,B=1)}{P(A=1)}=0.8\),

\(P(B=0|A=1)=\frac{P(A=1,B=0)}{P(A=1)}=0.2\)

\begin{align}
\phi_5(A=1,C=1)&=\sum_{B}P(B|A=1)\cdot P(D=1|B,C=1)\\
    &=P(B=1|A=1)\cdot P(D=1|B=1,C=1)+P(B=0|A=1)\cdot P(D=1|B=0,C=1)\\
    &=0.8*0.9+0.2*0.1=0.74
\end{align}

\begin{align}
\phi_5(A=1,C=0)&=\sum_{B}P(B|A=1)\cdot P(D=1|B,C=0)\\
    &=P(B=1|A=1)\cdot P(D=1|B=1,C=0)+P(B=0|A=1)\cdot P(D=1|B=0,C=0)\\
    &=0.8*0.2+0.2*0.01=0.162
\end{align}

\begin{align}
\phi_5(A=0,C=0)&=\sum_{B}P(B|A=0)\cdot P(D=1|B,C=0)\\
    &=P(B=1|A=0)\cdot P(D=1|B=1,C=0)+P(B=0|A=0)\cdot P(D=1|B=0,C=0)\\
    &=0.1*0.2+0.9*0.01=0.029
\end{align}

\begin{align}
\phi_5(A=0,C=1)&=\sum_{B}P(B|A=0)\cdot P(D=1|B,C=1)\\
    &=P(B=1|A=0)\cdot P(D=1|B=1,C=1)+P(B=0|A=0)\cdot P(D=1|B=0,C=1)\\
    &=0.1*0.9+0.9*0.1=0.18
\end{align}


\textbf{消去 \( C \)}：
\[
\phi_6(A) = \sum_{C} \phi_3(C, A) \cdot \phi_5(A, C) = \sum_{C} P(C|A) \cdot \left( \sum_{B} P(B|A) \cdot P(D=1|B,C) \right)
\]

即\[\phi_6(A)=\sum_{C}P(C|A)\cdot \phi_5(A,C)\]
\begin{align}
    \phi_6(A=1)&=P(C=1|A=1)\cdot \phi_5(1,1)+P(C=0|A=1)\cdot\phi_5(1,0)\\
    &=0.9*0.74+0.1*0.162\\
    &=0.6822
\end{align}

\begin{align}
    \phi_6(A=0)&=P(C=1|A=0)\cdot \phi_5(0,1)+P(C=0|A=0)\cdot\phi_5(0,0)\\
    &=0.5*0.18+0.5*0.029\\
    &=0.1045
\end{align}


\textbf{计算 \( P(A, D=1) \)}：
\[
P(A, D=1) = \phi_1(A) \cdot \phi_6(A) = P(A) \cdot \sum_{C} P(C|A) \cdot \left( \sum_{B} P(B|A) \cdot P(D=1|B,C) \right)
\]
即：\[P(A=1,D=1)=0.7*0.6822=0.47754\]
\[P(A=0,D=1)=0.3*0.1045=0.03135\]

\textbf{归一化}：

\begin{align}
    P(A=1|D=1)&=\frac{ P(A=1,D=1) }{P(D=1)}\\
    &=\frac{P(A=1,D=1)}{P(A=1,D=1)+P(A=0,D=1)}\\
    &=\frac{0.47754}{0.47754+0.03135}\\
    &=0.96967
\end{align}


\section*{第5题}

\subsection*{可扩展性}
\begin{itemize}
    \item \textbf{变分推断}：
    \begin{itemize}
        \item \textbf{优点}：计算效率高，适合大规模数据集和复杂模型。通过优化问题将推断转化为确定性优化，计算成本较低，尤其在大数据场景下（如深度学习中的变分自编码器）。
        \item \textbf{缺点}：需要选择合适的变分分布，可能限制模型表达能力。
    \end{itemize}
    \item \textbf{MCMC}：
    \begin{itemize}
        \item \textbf{优点}：理论上适用于任何概率模型，灵活性高。
        \item \textbf{缺点}：计算成本高，采样过程耗时，尤其在大规模数据集或高维模型中，扩展性较差。
    \end{itemize}
\end{itemize}

\subsection*{收敛性}
\begin{itemize}
    \item \textbf{变分推断}：
    \begin{itemize}
        \item \textbf{优点}：基于优化（如梯度下降），收敛速度较快，通常能快速得到近似解。
        \item \textbf{缺点}：可能陷入局部最优，收敛到次优解，且依赖变分分布的选择。
    \end{itemize}
    \item \textbf{MCMC}：
    \begin{itemize}
        \item \textbf{优点}：理论上在无限样本下保证收敛到真实后验分布（全局最优）。
        \item \textbf{缺点}：收敛速度慢，需大量样本，实际中难以判断是否完全收敛，且可能受初始值或采样方案影响。
    \end{itemize}
\end{itemize}

\subsection*{近似偏差}
\begin{itemize}
    \item \textbf{变分推断}：
    \begin{itemize}
        \item \textbf{优点}：通过最小化 KL 散度提供一致的近似框架，适合快速近似。
        \item \textbf{缺点}：由于变分分布的限制（如均值场假设），近似偏差较大，可能低估后验方差或无法捕捉复杂后验结构。
    \end{itemize}
    \item \textbf{MCMC}：
    \begin{itemize}
        \item \textbf{优点}：无近似偏差，长期运行下可精确采样真实后验分布。
        \item \textbf{缺点}：实际中有限样本可能引入偏差，且计算复杂度高。
    \end{itemize}
\end{itemize}

变分推断的目标是最大化证据下界（ELBO），其表达式为：
\[
\text{ELBO}(q) = \mathbb{E}_{q(\theta)}[\log p(\mathbf{x}, \theta)] - \mathbb{E}_{q(\theta)}[\log q(\theta)] = \mathbb{E}_{q(\theta)}[\log p(\mathbf{x} | \theta)] - \text{KL}(q(\theta) || p(\theta))
\]
其中：
\begin{itemize}
    \item \( q(\theta) \)：变分分布，近似真实后验 \( p(\theta | \mathbf{x}) \)。
    \item \( p(\mathbf{x}, \theta) \)：联合分布，包含观测数据 \( \mathbf{x} \) 和参数 \( \theta \)。
    \item \( p(\mathbf{x} | \theta) \)：似然函数。
    \item \( p(\theta) \)：先验分布。
    \item \( \text{KL}(q(\theta) || p(\theta)) \)：变分分布与先验的 KL 散度。
\end{itemize}

ELBO 是对数边际似然 \( \log p(\mathbf{x}) \) 的下界，最大化 ELBO 等价于最小化 \( \text{KL}(q(\theta) || p(\theta | \mathbf{x})) \)。

\end{document}
